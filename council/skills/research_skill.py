from typing import Dict, Any, List, Optional, Callable, Awaitable
from pydantic import BaseModel, Field
import asyncio
import logging
from .base_skill import BaseSkill
from council.observability.tracer import AgentTracer

logger = logging.getLogger(__name__)


class ResearchInput(BaseModel):
    """ç ”ç©¶ä»»åŠ¡è¾“å…¥"""

    topic: str = Field(..., description="ç ”ç©¶ä¸»é¢˜")
    depth: int = Field(3, description="ç ”ç©¶æ·±åº¦ (æµè§ˆé¡µé¢æ•°)", ge=1, le=10)


class ResearchOutput(BaseModel):
    """ç ”ç©¶ä»»åŠ¡è¾“å‡º"""

    topic: str
    sources: List[str]
    summary: str
    metadata: Dict[str, Any] = Field(default_factory=dict)


class ResearchSkill(BaseSkill):
    """
    ç ”ç©¶æŠ€èƒ½ (ResearchSkill)

    æµç¨‹:
    1. æœç´¢ç›¸å…³ä¿¡æ¯ (Search)
    2. æµè§ˆå…³é”®é¡µé¢ (Browse)
    3. æ±‡æ€»ç”ŸæˆæŠ¥å‘Š (Summarize)

    Features:
    - Pydantic ç±»åž‹æ£€æŸ¥
    - OpenTelemetry è¿½è¸ª
    - ä¾èµ–æ³¨å…¥
    - é”™è¯¯å¤„ç†ä¸Žé‡è¯•
    """

    def __init__(
        self,
        llm_client=None,
        search_tool: Optional[Callable[[str], Awaitable[List[str]]]] = None,
        browse_tool: Optional[Callable[[str], Awaitable[str]]] = None,
        tracer: Optional[AgentTracer] = None,
        approval_callback: Optional[Callable] = None,
        progress_callback: Optional[Callable] = None,
    ):
        super().__init__(
            name="ResearchSkill",
            description="Deep research on a topic using search and browse tools",
            llm_client=llm_client,
            approval_callback=approval_callback,
            progress_callback=progress_callback,
        )
        self.tools = {
            "search": search_tool or self._mock_search,
            "browse": browse_tool or self._mock_browse,
        }
        self.tracer = tracer or AgentTracer("research-skill")

    async def execute(self, topic: str, depth: int = 3, **kwargs) -> Dict[str, Any]:
        """
        æ‰§è¡Œç ”ç©¶ä»»åŠ¡

        Args:
            topic: ç ”ç©¶ä¸»é¢˜
            depth: ç ”ç©¶æ·±åº¦
        """
        # 1. éªŒè¯è¾“å…¥
        try:
            input_data = ResearchInput(topic=topic, depth=depth)
        except Exception as e:
            logger.error(f"Input validation failed: {e}")
            raise ValueError(f"Invalid input: {e}")

        with self.tracer.trace_agent_step("ResearchSkill", "execute") as span:
            span.set_attribute("topic", input_data.topic)
            span.set_attribute("depth", input_data.depth)

            logger.info(f"ðŸ” [ResearchSkill] å¼€å§‹ç ”ç©¶: {input_data.topic}")

            try:
                # 2. æœç´¢
                with self.tracer.trace_tool_call("search", {"query": input_data.topic}):
                    search_results = await self.tools["search"](input_data.topic)

                logger.info(f"ðŸ“‘ [ResearchSkill] æ‰¾åˆ° {len(search_results)} ä¸ªç›¸å…³ç»“æžœ")

                if not search_results:
                    return ResearchOutput(
                        topic=input_data.topic,
                        sources=[],
                        summary="No sources found.",
                        metadata={"status": "no_results"},
                    ).model_dump()

                # 3. æµè§ˆ (å¹¶å‘)
                tasks = []
                target_urls = search_results[: input_data.depth]

                for url in target_urls:
                    tasks.append(self._safe_browse(url))

                contents = await asyncio.gather(*tasks)
                valid_contents = [c for c in contents if c]

                logger.info(
                    f"ðŸ“– [ResearchSkill] å·²æµè§ˆ {len(valid_contents)}/{len(target_urls)} ä¸ªé¡µé¢"
                )

                # 4. æ€»ç»“
                with self.tracer.trace_llm_call(
                    "summarizer", f"Summarize {len(valid_contents)} sources"
                ):
                    summary = await self._summarize(input_data.topic, valid_contents)

                # 5. æž„é€ è¾“å‡º
                output = ResearchOutput(
                    topic=input_data.topic,
                    sources=target_urls,
                    summary=summary,
                    metadata={
                        "total_sources": len(search_results),
                        "browsed_count": len(valid_contents),
                    },
                )

                return output.model_dump()

            except Exception as e:
                logger.error(f"Research failed: {e}", exc_info=True)
                span.set_attribute("error", str(e))
                raise RuntimeError(f"Research execution failed: {e}")

    async def _safe_browse(self, url: str) -> Optional[str]:
        """å®‰å…¨æµè§ˆï¼Œå¸¦é”™è¯¯å¤„ç†"""
        try:
            with self.tracer.trace_tool_call("browse", {"url": url}):
                return await self.tools["browse"](url)
        except Exception as e:
            logger.warning(f"Failed to browse {url}: {e}")
            return None

    async def _mock_search(self, query: str) -> List[str]:
        """æ¨¡æ‹Ÿæœç´¢å·¥å…·"""
        await asyncio.sleep(0.5)
        return [f"https://example.com/result{i}?q={query}" for i in range(1, 6)]

    async def _mock_browse(self, url: str) -> str:
        """æ¨¡æ‹Ÿæµè§ˆå·¥å…·"""
        await asyncio.sleep(0.5)
        return f"Content from {url}: This is relevant information about the topic."

    async def _summarize(self, topic: str, contents: List[str]) -> str:
        """æ€»ç»“å†…å®¹"""
        if not contents:
            return "No content available to summarize."

        if self.llm_client:
            # å®žé™… LLM è°ƒç”¨é€»è¾‘
            # é™åˆ¶æ¯ä¸ªæ¥æºçš„é•¿åº¦ï¼Œé˜²æ­¢ Token çˆ†ç‚¸
            truncated_contents = []
            total_chars = 0
            MAX_CHARS_PER_SOURCE = 2000
            MAX_TOTAL_CHARS = 10000

            for c in contents:
                if total_chars >= MAX_TOTAL_CHARS:
                    break

                truncated = c[:MAX_CHARS_PER_SOURCE]
                if len(c) > MAX_CHARS_PER_SOURCE:
                    truncated += "...(truncated)"

                truncated_contents.append(truncated)
                total_chars += len(truncated)

            prompt = (
                f"Topic: {topic}\n\nSources:\n"
                + "\n".join(truncated_contents)
                + "\n\nSummarize:"
            )
            complete = getattr(self.llm_client, "complete", None)
            if not callable(complete):
                raise NotImplementedError("llm_client must provide complete()")
            result = complete(prompt)
            if asyncio.iscoroutine(result):
                result = await result
            return result

        return f"Auto-generated summary for {topic}: Found {len(contents)} relevant sources containing key insights."
