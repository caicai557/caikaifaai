"""
Data Reducer - æ•°æ®é™ç»´å™¨

æ ¸å¿ƒåŠŸèƒ½:
1. æ‰§è¡Œå±‚æ•°æ®é™ç»´ï¼Œä»…è¿”å›é«˜ä¿¡å·æ‘˜è¦
2. 10MB æ—¥å¿— â†’ â‰¤2KB æ‘˜è¦
3. PII æ•°æ®è‡ªåŠ¨è¿‡æ»¤
4. å¼‚å¸¸æ£€æµ‹ä¸æå–

è§„åˆ™:
- æœ€å¤§è¾“å‡º 2000 å­—ç¬¦
- ä¿ç•™å…³é”®ç»Ÿè®¡ä¿¡æ¯
- ç§»é™¤å†—ä½™å†…å®¹
"""

from dataclasses import dataclass
from typing import List, Optional, Dict, Any
from enum import Enum
import re


class AnomalyType(Enum):
    """å¼‚å¸¸ç±»å‹"""

    ERROR = "error"
    WARNING = "warning"
    CRITICAL = "critical"
    PERFORMANCE = "performance"
    SECURITY = "security"


@dataclass
class Anomaly:
    """å¼‚å¸¸ä¿¡æ¯"""

    type: AnomalyType
    description: str
    line_number: Optional[int] = None
    context: Optional[str] = None
    severity: int = 1  # 1-10


class DataReducer:
    """
    æ•°æ®é™ç»´å™¨ - ä»…è¿”å›é«˜ä¿¡å·æ‘˜è¦

    æ ¸å¿ƒè§„åˆ™:
    - 10MB æ—¥å¿— â†’ â‰¤2KB æ‘˜è¦
    - 10,000 è¡Œæ•°æ® â†’ å…³é”®ç»Ÿè®¡ + å¼‚å¸¸
    - PII æ•°æ®è‡ªåŠ¨è¿‡æ»¤
    """

    # PII æ¨¡å¼
    PII_PATTERNS: List[tuple] = [
        (r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b", "[EMAIL]"),
        (r"\b\d{3}[-.]?\d{3}[-.]?\d{4}\b", "[PHONE]"),
        (r"\b\d{3}-\d{2}-\d{4}\b", "[SSN]"),
        (r"\b(?:4[0-9]{12}(?:[0-9]{3})?|5[1-5][0-9]{14})\b", "[CREDIT_CARD]"),
        (r"\b\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\b", "[IP_ADDRESS]"),
        (r"(?i)password\s*[=:]\s*\S+", "[PASSWORD_REDACTED]"),
        (r"(?i)api[_-]?key\s*[=:]\s*\S+", "[API_KEY_REDACTED]"),
        (r"(?i)secret\s*[=:]\s*\S+", "[SECRET_REDACTED]"),
        (r"(?i)token\s*[=:]\s*\S+", "[TOKEN_REDACTED]"),
    ]

    # å¼‚å¸¸æ£€æµ‹æ¨¡å¼
    ANOMALY_PATTERNS = [
        (r"(?i)\berror\b", AnomalyType.ERROR),
        (r"(?i)\bwarning\b", AnomalyType.WARNING),
        (r"(?i)\bcritical\b", AnomalyType.CRITICAL),
        (r"(?i)\bfailed\b", AnomalyType.ERROR),
        (r"(?i)\bexception\b", AnomalyType.ERROR),
        (r"(?i)\btimeout\b", AnomalyType.PERFORMANCE),
        (r"(?i)\bunauthorized\b", AnomalyType.SECURITY),
        (r"(?i)\bdenied\b", AnomalyType.SECURITY),
    ]

    def __init__(
        self,
        max_chars: int = 2000,
        filter_pii: bool = True,
        extract_stats: bool = True,
    ):
        self.max_chars = max_chars
        self.filter_pii = filter_pii
        self.extract_stats = extract_stats

        # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
        self._pii_compiled = [
            (re.compile(pattern), replacement)
            for pattern, replacement in self.PII_PATTERNS
        ]
        self._anomaly_compiled = [
            (re.compile(pattern), anomaly_type)
            for pattern, anomaly_type in self.ANOMALY_PATTERNS
        ]

    def reduce(
        self,
        stdout: str,
        stderr: str = "",
        max_tokens: Optional[int] = None,
    ) -> str:
        """
        å‹ç¼©è¾“å‡ºä¸ºæ‘˜è¦

        Args:
            stdout: æ ‡å‡†è¾“å‡º
            stderr: æ ‡å‡†é”™è¯¯
            max_tokens: æœ€å¤§ token æ•° (å¯é€‰)

        Returns:
            å‹ç¼©åçš„æ‘˜è¦ (â‰¤2000 å­—ç¬¦)
        """
        max_chars = max_tokens or self.max_chars

        # Step 1: è¿‡æ»¤ PII
        if self.filter_pii:
            stdout = self._filter_pii(stdout)
            stderr = self._filter_pii(stderr)

        # Step 2: æå–å…³é”®ä¿¡æ¯
        combined = self._combine_output(stdout, stderr)

        # Step 3: å¦‚æœè¶³å¤ŸçŸ­ï¼Œç›´æ¥è¿”å›
        if len(combined) <= max_chars:
            return combined

        # Step 4: æ™ºèƒ½å‹ç¼©
        summary = self._smart_compress(combined, max_chars)

        return summary

    def extract_anomalies(self, data: str) -> List[Anomaly]:
        """
        æå–å…³é”®å¼‚å¸¸ä¿¡æ¯

        Args:
            data: è¦åˆ†æçš„æ•°æ®

        Returns:
            æ£€æµ‹åˆ°çš„å¼‚å¸¸åˆ—è¡¨
        """
        anomalies = []
        lines = data.split("\n")

        for i, line in enumerate(lines, 1):
            for pattern, anomaly_type in self._anomaly_compiled:
                if pattern.search(line):
                    # æå–ä¸Šä¸‹æ–‡
                    context_start = max(0, i - 2)
                    context_end = min(len(lines), i + 2)
                    context = "\n".join(lines[context_start:context_end])

                    anomaly = Anomaly(
                        type=anomaly_type,
                        description=line.strip()[:200],  # é™åˆ¶é•¿åº¦
                        line_number=i,
                        context=context[:500],
                        severity=self._calculate_severity(anomaly_type),
                    )
                    anomalies.append(anomaly)

        # å»é‡å¹¶æ’åº
        unique_anomalies = self._deduplicate_anomalies(anomalies)
        return sorted(unique_anomalies, key=lambda a: a.severity, reverse=True)[:20]

    def extract_statistics(self, data: str) -> Dict[str, Any]:
        """
        æå–ç»Ÿè®¡ä¿¡æ¯

        Args:
            data: è¦åˆ†æçš„æ•°æ®

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        lines = data.split("\n")

        stats = {
            "total_lines": len(lines),
            "total_chars": len(data),
            "error_count": 0,
            "warning_count": 0,
            "unique_patterns": set(),
        }

        for line in lines:
            if re.search(r"(?i)\berror\b", line):
                stats["error_count"] += 1
            if re.search(r"(?i)\bwarning\b", line):
                stats["warning_count"] += 1

        # è½¬æ¢ set ä¸º list ä»¥ä¾¿ JSON åºåˆ—åŒ–
        stats["unique_patterns"] = list(stats["unique_patterns"])

        return stats

    def _filter_pii(self, text: str) -> str:
        """è¿‡æ»¤ PII æ•°æ®"""
        for pattern, replacement in self._pii_compiled:
            text = pattern.sub(replacement, text)
        return text

    def _combine_output(self, stdout: str, stderr: str) -> str:
        """åˆå¹¶è¾“å‡º"""
        # ä¼˜åŒ–: å¦‚æœåªæœ‰ stdout ä¸”æ²¡æœ‰ stderrï¼Œç›´æ¥è¿”å› stdout (é¿å… header å¢åŠ  token)
        if stdout and not stderr:
            return stdout.strip()

        parts = []

        if stdout.strip():
            parts.append(f"=== STDOUT ===\n{stdout.strip()}")

        if stderr.strip():
            parts.append(f"=== STDERR ===\n{stderr.strip()}")

        return "\n\n".join(parts) if parts else "(æ— è¾“å‡º)"

    def _smart_compress(self, text: str, max_chars: int) -> str:
        """æ™ºèƒ½å‹ç¼©"""
        lines = text.split("\n")

        # ç­–ç•¥ 1: ä¿ç•™é¦–å°¾ + å…³é”®è¡Œ
        important_lines = []

        # ä¿ç•™å‰ 20 è¡Œ
        important_lines.extend(lines[:20])

        # ä¿ç•™åŒ…å«å…³é”®è¯çš„è¡Œ
        keywords = ["error", "warning", "failed", "success", "result", "total", "count"]
        for line in lines[20:-20]:
            if any(kw in line.lower() for kw in keywords):
                important_lines.append(line)

        # ä¿ç•™å 10 è¡Œ
        important_lines.extend(lines[-10:])

        # ç»„è£…æ‘˜è¦
        summary_text = "\n".join(important_lines)

        # å¦‚æœè¿˜æ˜¯å¤ªé•¿ï¼Œå¼ºåˆ¶æˆªæ–­
        if len(summary_text) > max_chars:
            truncated = summary_text[: max_chars - 100]
            summary_text = f"{truncated}\n\n... [æˆªæ–­ï¼ŒåŸå§‹ {len(text)} å­—ç¬¦]"

        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        if self.extract_stats:
            stats = self.extract_statistics(text)
            stats_line = f"\nğŸ“Š ç»Ÿè®¡: {stats['total_lines']} è¡Œ, {stats['error_count']} é”™è¯¯, {stats['warning_count']} è­¦å‘Š"
            if len(summary_text) + len(stats_line) <= max_chars:
                summary_text += stats_line

        return summary_text

    def _calculate_severity(self, anomaly_type: AnomalyType) -> int:
        """è®¡ç®—å¼‚å¸¸ä¸¥é‡ç¨‹åº¦"""
        severity_map = {
            AnomalyType.CRITICAL: 10,
            AnomalyType.SECURITY: 9,
            AnomalyType.ERROR: 7,
            AnomalyType.WARNING: 4,
            AnomalyType.PERFORMANCE: 5,
        }
        return severity_map.get(anomaly_type, 1)

    def _deduplicate_anomalies(self, anomalies: List[Anomaly]) -> List[Anomaly]:
        """å»é‡å¼‚å¸¸"""
        seen = set()
        unique = []

        for anomaly in anomalies:
            key = (anomaly.type, anomaly.description[:50])
            if key not in seen:
                seen.add(key)
                unique.append(anomaly)

        return unique


__all__ = ["DataReducer", "Anomaly", "AnomalyType"]
