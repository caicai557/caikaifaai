#!/usr/bin/env python3
"""
AI Council å¼€å‘èµ„æ–™æ–‡æ¡£ç”Ÿæˆå™¨ v2
ä½¿ç”¨åˆ†æ®µæ–¹å¼é¿å…å¤§å­—ç¬¦ä¸²åµŒå¥—é—®é¢˜
"""

import json
from datetime import datetime
from pathlib import Path
from typing import Dict


def collect_metadata(council_dir: Path) -> Dict:
    """æ”¶é›†æ–‡æ¡£å…ƒæ•°æ®"""
    metadata = {
        "generated_at": datetime.now().isoformat(),
        "documents": {},
        "stats": {"total_docs": 0, "prompts_count": 0, "routines_count": 0},
    }

    for md_file in council_dir.glob("*.md"):
        metadata["documents"][md_file.name] = {
            "path": str(md_file),
            "size_kb": round(md_file.stat().st_size / 1024, 2),
            "modified": datetime.fromtimestamp(md_file.stat().st_mtime).isoformat(),
        }
        metadata["stats"]["total_docs"] += 1

    prompts_dir = council_dir / "prompts"
    if prompts_dir.exists():
        metadata["stats"]["prompts_count"] = len(list(prompts_dir.glob("*.md")))

    routines_dir = council_dir / "routines"
    if routines_dir.exists():
        metadata["stats"]["routines_count"] = len(list(routines_dir.glob("*.py")))

    return metadata


def generate_index() -> str:
    """ç”Ÿæˆç´¢å¼•æ–‡æ¡£"""
    now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    parts = []
    parts.append(f"# AI Council å¼€å‘èµ„æ–™ä¸­å¿ƒ\n\n> è‡ªåŠ¨ç”Ÿæˆäº {now}\n\n")
    parts.append("## ğŸ“š æ–‡æ¡£å¯¼èˆª\n\n")
    parts.append("### ğŸ¯ æ ¸å¿ƒæ¶æ„\n\n")
    parts.append("| æ–‡æ¡£ | è¯´æ˜ | çŠ¶æ€ |\n")
    parts.append("|------|------|------|\n")
    parts.append("| [AGENTS.md](../AGENTS.md) | Agent æ²»ç†å®ªæ³• | âœ… |\n")
    parts.append("| [CODEMAP.md](../../CODEMAP.md) | é¡¹ç›®ä»£ç åœ°å›¾ | âœ… |\n")
    parts.append("| [SOP.md](../SOP.md) | å…­æ­¥è‡ªæ„ˆå¾ªç¯ SOP | âœ… |\n")
    parts.append("| [DECISIONS.md](../DECISIONS.md) | æ¶æ„å†³ç­–æ—¥å¿— | âœ… |\n\n")

    parts.append("### ğŸ”§ æœ€ä½³å®è·µ\n\n")
    parts.append("| æ–‡æ¡£ | è¯´æ˜ |\n")
    parts.append("|------|------|\n")
    parts.append(
        "| [TOKEN_SAVING_PRACTICES.md](../TOKEN_SAVING_PRACTICES.md) | Token ä¼˜åŒ– |\n"
    )
    parts.append("| [MCP_PHILOSOPHY.md](../MCP_PHILOSOPHY.md) | MCP åè®®ç†å¿µ |\n")
    parts.append(
        "| [MCP_BEST_PRACTICES.md](../MCP_BEST_PRACTICES.md) | MCP å®æ“æŒ‡å— |\n\n"
    )

    parts.append("### ğŸ¤– æ¨¡å‹ä¸“ç”¨æŒ‡å—\n\n")
    parts.append("| æ–‡æ¡£ | ç›®æ ‡æ¨¡å‹ |\n")
    parts.append("|------|----------|\n")
    parts.append("| [CLAUDE.md](../CLAUDE.md) | Claude Opus 4.5 |\n")
    parts.append("| [CODEX.md](../CODEX.md) | Codex 5.2 |\n")
    parts.append("| [GEMINI.md](../GEMINI.md) | Gemini Pro/Flash |\n\n")

    parts.append("## ğŸš€ å¿«é€Ÿå¼€å§‹\n\n")
    parts.append("```bash\n")
    parts.append("# éªŒè¯é—¨ç¦\n")
    parts.append("just verify\n\n")
    parts.append("# å…­æ­¥æµç¨‹\n")
    parts.append('/plan "éœ€æ±‚"    # 1. PM è§„åˆ’\n')
    parts.append('/audit "æ¨¡å—"   # 2. æ¶æ„å®¡è®¡\n')
    parts.append('/tdd "èŒƒå›´"     # 3. TDD\n')
    parts.append('/impl "èŒƒå›´"    # 4. å®ç°\n')
    parts.append("just verify      # 5. è£å†³\n")
    parts.append("/review          # 6. å®¡æŸ¥\n")
    parts.append("```\n\n")

    parts.append("## ğŸ¯ æ¨¡å‹è·¯ç”±ç­–ç•¥\n\n")
    parts.append("| æ¨¡å‹ | å æ¯” | åœºæ™¯ |\n")
    parts.append("|------|------|------|\n")
    parts.append("| Claude Opus 4.5 | 5% | è§„åˆ’æ€»æ§ |\n")
    parts.append("| Codex 5.2 | 10% | ä»£ç å®¡æŸ¥ |\n")
    parts.append("| Gemini 3 Pro | 5% | æ¶æ„å®¡è®¡ |\n")
    parts.append("| Gemini 3 Flash | 80% | å¿«é€Ÿå®ç° |\n\n")

    parts.append("---\n\n")
    parts.append(f"**æœ€åæ›´æ–°**: {now}\n")

    return "".join(parts)


def generate_best_practices() -> str:
    """ç”Ÿæˆæœ€ä½³å®è·µæ–‡æ¡£"""
    now = datetime.now().strftime("%Y-%m-%d")

    parts = []
    parts.append("# AI Council æœ€ä½³å®è·µ (2025)\n\n")
    parts.append("> åŸºäºè¡Œä¸šæœ€æ–°ç ”ç©¶\n\n")

    parts.append("## ğŸ¯ æ ¸å¿ƒå‘ç°\n\n")
    parts.append("æ ¹æ® Anthropic å†…éƒ¨ç ”ç©¶ï¼š\n\n")
    parts.append("- **90.2% æ€§èƒ½æå‡**: å¤šæ™ºèƒ½ä½“ vs å•æ™ºèƒ½ä½“\n")
    parts.append("- **32.3% Token å‰Šå‡**: é€šè¿‡æ¨¡å‹åˆ†å±‚\n")
    parts.append("- **2.8-4.4x é€Ÿåº¦**: å¹¶è¡Œåè°ƒ\n\n")

    parts.append("## ğŸ—ï¸ æ¶æ„æ¨¡å¼\n\n")
    parts.append("### Orchestrator-Worker Pattern\n\n")
    parts.append("```\n")
    parts.append("Orchestrator (Opus/Gemini Pro)\n")
    parts.append("    â”œâ”€> Worker 1 (Sonnet/Flash)\n")
    parts.append("    â”œâ”€> Worker 2 (Sonnet/Flash)\n")
    parts.append("    â””â”€> Worker 3 (Sonnet/Flash)\n")
    parts.append("```\n\n")

    parts.append("**åŸåˆ™**:\n")
    parts.append("- Orchestrator: è§„åˆ’+è·¯ç”±ï¼ˆåªè¯»æƒé™ï¼‰\n")
    parts.append("- Workers: å•ä¸€ä»»åŠ¡ï¼ˆçª„æƒé™ï¼‰\n")
    parts.append("- å°æ¨¡å‹æ‰§è¡Œï¼Œå¤§æ¨¡å‹åè°ƒ\n\n")

    parts.append("### Hub-and-Spoke äº‹ä»¶æ¶æ„\n\n")
    parts.append("è§£è€¦æ™ºèƒ½ä½“é€šä¿¡ï¼Œå¤æ‚åº¦ä» O(NÂ²) é™åˆ° O(N)\n\n")

    parts.append("### ç¨‹åºåŒ–å·¥å…·è°ƒç”¨ (PTC)\n\n")
    parts.append("Token èŠ‚çœçº¦ 98.7%ï¼š\n\n")
    parts.append("```python\n")
    parts.append("# ç¼–å†™è„šæœ¬æ‰¹é‡å¤„ç†ï¼Œæ›¿ä»£è‡ªç„¶è¯­è¨€å¾ªç¯\n")
    parts.append("import glob\n")
    parts.append("for f in glob.glob('data/*.json'):\n")
    parts.append("    process(f)\n")
    parts.append("```\n\n")

    parts.append("## ğŸ”€ æ¨¡å‹é€‰æ‹©\n\n")
    parts.append("| æ¨¡å‹ | ä»»åŠ¡åˆ†è§£ | ç¨³å®šæ€§ | æ¨èåœºæ™¯ |\n")
    parts.append("|------|---------|--------|----------|\n")
    parts.append("| Claude | â­â­â­â­â­ | â­â­â­ | è§„åˆ’ã€ç”Ÿæˆ |\n")
    parts.append("| Gemini | â­â­â­â­ | â­â­â­â­ | åè°ƒã€å®¡è®¡ |\n\n")

    parts.append("## ğŸ“ å§”æ‰˜æœ€ä½³å®è·µ\n\n")
    parts.append("### é”™è¯¯ç¤ºä¾‹ âŒ\n\n")
    parts.append("```\n")
    parts.append('"ç ”ç©¶åŠå¯¼ä½“çŸ­ç¼º"  # è¿‡äºæ¨¡ç³Š\n')
    parts.append("```\n\n")

    parts.append("### æ­£ç¡®ç¤ºä¾‹ âœ…\n\n")
    parts.append("```\n")
    parts.append("ä»»åŠ¡: æ”¶é›† 2023-2025 åŠå¯¼ä½“æ•°æ®\n")
    parts.append("ç›®æ ‡: åˆ†æä¾›åº”é“¾å½±å“\n")
    parts.append("è¾“å‡º: JSON æ ¼å¼\n")
    parts.append("å·¥å…·: WebSearch (é™ 3 æº)\n")
    parts.append("è¾¹ç•Œ: ä»…æ±½è½¦èŠ¯ç‰‡\n")
    parts.append("```\n\n")

    parts.append("## ğŸ”’ å®‰å…¨åŸåˆ™\n\n")
    parts.append("âš ï¸ **æƒé™è”“å»¶æ˜¯ä¸å®‰å…¨è‡ªä¸»æ€§çš„æœ€å¿«è·¯å¾„**\n\n")
    parts.append("- ä» deny-all å¼€å§‹\n")
    parts.append("- ä»…å…è®¸å¿…éœ€å‘½ä»¤\n")
    parts.append("- æ•æ„Ÿæ“ä½œéœ€ç¡®è®¤\n")
    parts.append("- é˜»æ­¢å±é™©å‘½ä»¤\n\n")

    parts.append("## âš¡ Token ä¼˜åŒ–\n\n")
    parts.append("### 1. æ¸è¿›å¼å·¥å…·åŠ è½½\n\n")
    parts.append("èŠ‚çœ ~95% åˆå§‹ä¸Šä¸‹æ–‡\n\n")
    parts.append("### 2. Session é¢„ç®—ç®¡ç†\n\n")
    parts.append("```\n")
    parts.append("200k é¢„ç®—åˆ†é…:\n")
    parts.append("- éœ€æ±‚ç†è§£: 10k\n")
    parts.append("- ä¿¡æ¯æŸ¥è¯¢: 15k\n")
    parts.append("- ä»£ç å®ç°: 20k\n")
    parts.append("- å®¡æŸ¥ä¿®å¤: 10k\n")
    parts.append("- é¢„ç•™: 140k\n")
    parts.append("```\n\n")

    parts.append("## ğŸ“š å‚è€ƒèµ„æº\n\n")
    parts.append("### å®˜æ–¹æ–‡æ¡£\n\n")
    parts.append(
        "- [Anthropic Multi-Agent Research](https://www.anthropic.com/engineering/multi-agent-research-system)\n"
    )
    parts.append(
        "- [Claude Agent SDK (2025)](https://skywork.ai/blog/claude-agent-sdk-best-practices-ai-agents-2025/)\n"
    )
    parts.append(
        "- [Azure AI Patterns](https://learn.microsoft.com/en-us/azure/architecture/ai-ml/guide/ai-agent-design-patterns)\n\n"
    )

    parts.append("### å¼€æºé¡¹ç›®\n\n")
    parts.append("- [claude-flow](https://github.com/ruvnet/claude-flow)\n")
    parts.append("- [ccswarm](https://github.com/nwiizo/ccswarm)\n")
    parts.append("- [wshobson/agents](https://github.com/wshobson/agents)\n\n")

    parts.append("---\n\n")
    parts.append(f"**æœ€åæ›´æ–°**: {now}\n")

    return "".join(parts)


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹ç”Ÿæˆ AI Council å¼€å‘èµ„æ–™æ–‡æ¡£...\n")

    council_dir = Path(".council")
    output_dir = council_dir / "docs"
    output_dir.mkdir(exist_ok=True)

    # 1. æ”¶é›†å…ƒæ•°æ®
    print("ğŸ“Š æ”¶é›†æ–‡æ¡£å…ƒæ•°æ®...")
    metadata = collect_metadata(council_dir)

    metadata_file = output_dir / "metadata.json"
    with open(metadata_file, "w", encoding="utf-8") as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    print(f"âœ… å…ƒæ•°æ®: {metadata_file}")

    # 2. ç”Ÿæˆç´¢å¼•
    print("\nğŸ“š ç”Ÿæˆç´¢å¼•æ–‡æ¡£...")
    index_content = generate_index()
    index_file = output_dir / "INDEX.md"
    with open(index_file, "w", encoding="utf-8") as f:
        f.write(index_content)
    print(f"âœ… ç´¢å¼•: {index_file}")

    # 3. ç”Ÿæˆæœ€ä½³å®è·µ
    print("\nğŸ¯ ç”Ÿæˆæœ€ä½³å®è·µ...")
    bp_content = generate_best_practices()
    bp_file = output_dir / "BEST_PRACTICES_2025.md"
    with open(bp_file, "w", encoding="utf-8") as f:
        f.write(bp_content)
    print(f"âœ… æœ€ä½³å®è·µ: {bp_file}")

    # 4. ç»Ÿè®¡
    print("\n" + "=" * 60)
    print("ğŸ“Š ç”Ÿæˆç»Ÿè®¡:")
    print(f"  - æ€»æ–‡æ¡£æ•°: {metadata['stats']['total_docs']}")
    print(f"  - Prompts: {metadata['stats']['prompts_count']}")
    print(f"  - Routines: {metadata['stats']['routines_count']}")
    print(f"  - è¾“å‡ºç›®å½•: {output_dir}")
    print("=" * 60)

    print("\nâœ¨ æ–‡æ¡£ç”Ÿæˆå®Œæˆï¼")
    print("\nğŸ“– å¿«é€Ÿè®¿é—®:")
    print(f"  - ç´¢å¼•: {index_file}")
    print(f"  - æœ€ä½³å®è·µ: {bp_file}")
    print(f"  - å…ƒæ•°æ®: {metadata_file}")


if __name__ == "__main__":
    main()
